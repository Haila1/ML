# -*- coding: utf-8 -*-
"""CSC462 Project Shared.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sxR842_Da781GSGWqRkCHtESmEgBvLx5
"""

#Import Librarys

from keras.datasets import fashion_mnist
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler 
from sklearn.decomposition import PCA 
from keras.models import Sequential
from keras.utils import to_categorical
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, multilabel_confusion_matrix
from sklearn import svm

# def load_data():X_train,y_train,X_test,y_test

def load_data():
  (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()
  class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

  plt.figure(figsize=(10,10))
  for i in range(25):
      plt.subplot(5,5,i+1)
      plt.xticks([])
      plt.yticks([])
      plt.grid(False)
      plt.imshow(X_train[i], cmap=plt.cm.binary)
      plt.xlabel(y_train[y_train[i]]) #Print the index of the label
      plt.xlabel(class_names[y_train[i]]) #Print the label
  plt.show()

  return X_train, y_train, X_test, y_test

#def normalize_data(X_train,X_test):X_train,X_test

def normalize_data(X_train, X_test):
  X_train = X_train / 255.0
  X_test = X_test / 255.0
  return X_train, X_test

#def extract_features(X_train,X_test):train_features,test_features

def extract_features(X_train, X_test):
  train_features = np.reshape(X_train,(60000, -1)) 
  test_features = np.reshape(X_test,(10000, -1)) 
  return train_features, test_features

#def reduce_features(features,train_features,test_features):train_features,test_features

def reduce_features(features,train_features,test_features):
  #Step 1: Standrize data 
  scalar = StandardScaler()
  scaled_train_features = pd.DataFrame(scalar.fit_transform(train_features))
  scaled_test_features = pd.DataFrame(scalar.fit_transform(test_features))

  #Step 2: Apply PCA
  pca = PCA(n_components = features)

  pca.fit(scaled_train_features)
  train_features = pca.transform(scaled_train_features)

  pca.fit(scaled_test_features)
  test_features = pca.transform(scaled_test_features)

  return train_features, test_features

#def encode_output(y_train,y_test):y_train,y_test

def encode_output(y_train, y_test):
  y_train = to_categorical(y_train) 
  y_test = to_categorical(y_test)

  return y_train, y_test

#def create_MLP(features,classes):model

def create_MLP(features, classes):
  
  model = Sequential()
  model.add(Dense(2048, input_shape=features, activation='relu'))
  model.add(Dense(classes, activation='softmax')) 
  model.compile(loss='categorical_crossentropy', optimizer='adam',  metrics=['accuracy'])

  return model

#def create_CNN(height,weight,channels,classes):model

def create_CNN(height, width, channels, classes):
 
  model = Sequential()
  model.add(Conv2D(28, (3, 3), activation='relu', input_shape=(height, width, channels), strides=(1, 1), padding="same"))
  model.add(MaxPooling2D((2, 2)))
  model.add(Conv2D(56, (3, 3), activation='relu'))
  model.add(MaxPooling2D((2, 2)))
  model.add(Flatten())
  model.add(Dense(56, activation='softmax'))
  model.add(Dense(classes))
  model.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])
  
  return model

#def create_SVM(c):model

def create_SVM(c): 
  model = svm.SVC(C=c, kernel='poly', degree=1)
  
  return model

#def evaluate_NN(X_test,y_test,model):accuracy,precision,recall,f1_score

def evaluate_NN(X_test,y_test,model):

  y_predict=model.predict(X_test)
  y_predict=to_categorical(np.argmax(y_predict,axis=-1), 10)
  accuracy = accuracy_score(y_test, y_predict)
  precision = precision_score(y_test, y_predict, average='macro')
  recall = recall_score(y_test, y_predict, average='macro')
  f1_score = 2 * ((recall * precision) / (recall + precision)) 
  return accuracy, precision, recall, f1_score

#def evaluate_SVM(X_test,y_test,model):accuracy,precision,recall,f1_score

def evaluate_SVM(X_test,y_test,model):
  
  y_predict=model.predict(X_test)
  accuracy = accuracy_score(y_test, y_predict)
  precision = precision_score(y_test, y_predict, average='macro')
  recall = recall_score(y_test, y_predict, average='macro')
  f1_score = 2 * ((recall * precision) / (recall + precision)) 

  return accuracy, precision, recall, f1_score

def main():

  # 1. Load fashion-MNIST dataset
  X_train, y_train, X_test, y_test = load_data()

  # 2. Normalize the loaded data
  X_train, X_test = normalize_data(X_train, X_test)

  # 3. Extract features from the loaded data
  train_features, test_features = extract_features(X_train, X_test)

  # 4. Reduce the features size to 100 by utilizing PCA technique.
  Rtrain_features, Rtest_features = reduce_features(100, train_features, test_features) 

  # 5. Encode the labels as one-hot representation
  y_train, y_test = encode_output(y_train, y_test)

  # 6. Define and train the following five models:
  # ▪ SVM with the original extracted features 
  SVM_model1 = create_SVM(1.0)
  SVM_model1_his = SVM_model1.fit(train_features, y_train.argmax(axis=1))
  SVM1_accuracy, SVM1_precision, SVM1_recall, SVM1_f1_score = evaluate_SVM(test_features, y_test.argmax(axis=1), SVM_model1)

  print('SVM: Extracted Features: \n')
  print('Accuracy: ', SVM1_accuracy)
  print('Precesion: ', SVM1_precision)
  print('F1 Score: ', SVM1_f1_score)
  print('Recall: ', SVM1_recall)


  print('-----------------------------------------------\n')
  # ▪ SVM with the reduced features
  SVM_model2 = create_SVM(1.0)
  SVM_model2_his = SVM_model2.fit(Rtrain_features, y_train.argmax(axis=1))
  SVM2_accuracy, SVM2_precision, SVM2_recall, SVM2_f1_score = evaluate_SVM(Rtest_features, y_test.argmax(axis=1),SVM_model2)
  
  print('SVM: Reduced Features: \n')
  print('Accuracy: ', SVM2_accuracy)
  print('Precesion: ', SVM2_precision)
  print('F1 Score: ', SVM2_f1_score)
  print('Recall: ', SVM2_recall)


  print('-----------------------------------------------\n')
  # ▪ MLP with the original extracted features 
  MLP_model1 = create_MLP(train_features.shape[1:], 10)
  MLP_Model1_his = MLP_model1.fit(train_features, y_train, batch_size=32, epochs=10)
  MLP1_accuracy, MLP1_precision, MLP1_recall, MLP1_f1_score = evaluate_NN(test_features, y_test, MLP_model1)

  print('MLP: Extracted Features: \n')
  print('Accuracy: ', MLP1_accuracy)
  print('Precesion: ', MLP1_precision)
  print('F1 Score: ', MLP1_f1_score)
  print('Recall: ', MLP1_recall)


  print('-----------------------------------------------\n')
  # ▪ MLP with the reduced features
  MLP_model2 = create_MLP(Rtrain_features.shape[1:], 10)
  MLP_Model2_his = MLP_model2.fit(Rtrain_features, y_train, batch_size=32, epochs=10)
  MLP2_accuracy, MLP2_precision, MLP2_recall, MLP2_f1_score = evaluate_NN(Rtest_features, y_test, MLP_model2)

  print('MLP: Reduced Features: \n')
  print('Accuracy: ', MLP2_accuracy)
  print('Precesion: ', MLP2_precision)
  print('F1 Score: ', MLP2_f1_score)
  print('Recall: ', MLP2_recall)


  print('-----------------------------------------------\n')
  # ▪ CNN with the normalized data (images)
  CNN_model1 = create_CNN(28, 28, 1, 10) 
  CNN_model1_his = CNN_model1.fit(X_train, y_train, batch_size=32, epochs=10)
  CNN_accuracy, CNN_precision, CNN_recall, CNN_f1_score = evaluate_NN(X_test, y_test, CNN_model1)

  print('CNN: \n')
  print('Accuracy: ', CNN_accuracy)
  print('Precesion: ', CNN_precision)
  print('F1 Score: ', CNN_f1_score)
  print('Recall: ', CNN_recall)


  # 7. Evaluate the trained models based on their accuracy, precision, recall, and f1-score

if __name__=="__main__":
 main()

